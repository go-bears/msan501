# Word similarity and relationships

## Goal

In this project, you will leverage a semi-recent advance in natural language processing called [word2vec](http://arxiv.org/pdf/1301.3781.pdf) (or just *word vectors*) to study the similarity between words. In particular, we're going to use a "database" from [Stanford's GloVe project](https://nlp.stanford.edu/projects/glove/).  For example, given a single word, we can find the *n* closest words:

```
Enter a word or 'x:y as z:'
> dog
dog is similar to {dogs cat pet puppy hound}
> cow
cow is similar to {cows mad bovine sheep goat}
> spain
spain is similar to {portugal spanish morocco madrid spaniards}
```

Given 3 words, we can also use word vector to fill in the blank of partial analogies of the form "*x is to y as z is to _____*":

```
Enter a word or 'x:y as z:'
> king:queen as man:
king is to queen as man is to {woman girl person teenager she}
> building:architect as software:
building is to architect as software is to {programmer architect designer computer microsoft}
> german:english as french:
german is to english as french is to {english welsh spanish language prohertrib}
> spanish:spain as french:
spanish is to spain as french is to {france belgium paris spain prohertrib}
```

Your goal is to implement a simple "shell" that repeatedly accepts either a word or a partial analogy in the form `x:y as z:` for 3 words x, y, and z.

## Description

Imagine trying to compare two documents for similarity. One document might be about "Installing Windows software" and another one might be about "Deinstalling Microsoft programs."  Because there are no words in common, at least for these titles, it's hard for a computer to tell these titles are related. A human, on the other hand, can easily equate Windows with Microsoft and software with programs etc., thus, finding the titles similar.

Until 2013, software could really only compare two words for exact match or a so-called *edit distance* (how many character edits to go from one word to the other). With word vector, we have a model for the "meaning" of a word in the form of a big vector of floats (usually 50 to 300 dimensional). These vectors are derived from a neural network that learns to map a word to an output vector such that neighboring words in some large corpus are close in 300-space. ("*The main intuition underlying the model is the simple observation that ratios of word-word co-occurrence probabilities have the potential for encoding some form of meaning.*" see [GloVe project](https://nlp.stanford.edu/projects/glove/)) For example, given the words `king`, `queen`, and `cat` here is a two-dimensional projection of the vectors for those words and the 4 nearest to those words (there is some overlap):

<img src="figures/wordvec1.png" width=400>

The amazing thing about these vectors is that somehow they really encode the relationship between words. From the original paper, which you can also verify with this project code, the vector arithmetic `king - man + woman` is extremely close to the vector for `queen`!  The GloVe project at Stanford has a nice example showing the vector difference between various words:

<img src="https://nlp.stanford.edu/projects/glove/images/man_woman.jpg" width=400>

A good place to start this project is the main program as it identifies the key sequence of operations.

### Main program

To provide a little commandline interpreter where users can type in words or partial analogies, you will use this main program:
 
```python
if __name__ == '__main__':
	glove_filename = sys.argv[1]
	gloves = load_glove(glove_filename)
	
	print("Enter a word or 'x:y as z:'")
	cmd = raw_input("> ")
	while cmd!=None:
	    match = re.search(r'(\w+):(\w+) as (\w+):', cmd)
	    if match is not None and len(match.groups())==3:
	        x = match.group(1).lower()
	        y = match.group(2).lower()
	        z = match.group(3).lower()
	        words = analogies(...)
	        print "%s is to %s as %s is to {%s}" % (x,y,z,' '.join(words))
	    elif re.match(r'\w+', cmd) is not None:
	        words = closest_words(...)
	        print "%s is similar to {%s}" % (cmd,' '.join(words))
	    else:
	        print("Enter a word or 'x:y as z:'")
	    cmd = raw_input("> ")
```

where you just have to fill in the arguments to `analogies(...)` and `closest_words(...)` and write those functions, as described below.

Users can kill the running program when they are done using control-C or can it control-D (on unix) to mean "end of file", thus, forcing the `raw_input()` to return `None`. That makes the loop terminate and therefore the program.

### Getting word vector data

The first thing your program needs to do is load the word vector "database" or table. Download the [6B tokens, 400K vocab, uncased, 50d, 100d, 200d, & 300d vectors, 822 MB download](http://nlp.stanford.edu/data/glove.6B.zip) file from the [GloVe project](https://nlp.stanford.edu/projects/glove) and unzip it. Save the resulting files somewhere useful like a `~/data/glove` directory in your home directory because it will be useful for other projects. There are files for different vector sizes (50, 100, 200, 300):

```bash
$ ls ~/data/glove/
glove.6B.100d.txt  glove.6B.200d.txt  glove.6B.300d.txt  glove.6B.50d.txt
```

You will be passing one of these filenames as an argument to the Python program. All of the examples in this document have used the 300-dimensional vectors. 

The format of the file is extremely simple: it's just the word followed by the components of the word vector. For example:

```
the 0.418 0.24968 -0.41242 0.1217 0.34527 -0.044457 -0.49688 ...
of 0.70853 0.57088 -0.4716 0.18048 0.54449 0.72603 0.18157 ...
to 0.68047 -0.039263 0.30186 -0.17792 0.42962 0.032246 ...
...
```

Use the following template to create a function that reads in one of these word vector files and returns a dictionary mapping words to vectors (of type numpy `array`).
 
```python
def load_glove(filename):
    """
    Read all lines from the indicated file and return a dictionary
    mapping word:vector where vectors are of numpy `array` type.
    GloVe file lines are of the form:

    the 0.418 0.24968 -0.41242 0.1217 ...

    So split each line on spaces into a list; the first element is the word
    and the remaining elements represent factor components. The length of the vector
    should not matter; read vectors of any length.
    """
    ...
```

### Computing similar words

Given a word, *w*, the easy way to find the *n* nearest words is to exhaustively compute the distance from *w*'s vector to every other vector in the database. Sort by the distance and take the first *n* words. Here is the signature of the function you must implement and a comment describing its implementation:

```python
def closest_words(gloves, word, n):
	"""
	Given a gloves dictionary of word:vector and a word return the n nearest words
	as a list of strings. The word is not considered his own nearest neighbor,
	so do not include that in the returned list.
	
	Compute the Euclidean distance between the vector for word and
	every other word's vector. Track the distances with a list of tuples
	of the form: (distance, word).  Sort the list by distance. Return a list
	of the first n words from the sorted list. Do not return the tuples, just the words. Return a python list of strings not numpy array.
	"""
	...
```

Given input `mexico`, your program should respond with:

```bash
mexico is similar to {mexican guatemala peru colombia america}
```

### Computing missing analogy words

Your final goal is to finish partial analogies given to you by the user. In other words, given input `building:architect as software:`, your program should respond with:

```bash
building is to architect as software is to {programmer architect designer computer microsoft}
```

The key is to look at the relationship between words, which means vector difference. Take a look at the following 2D projection of some word vectors and the vector differences, such as France-Paris and Spain-Madrid.

<img src="https://deeplearning4j.org/img/countries_capitals.png" width=450>

In 2D, the vector differences all are semi-flat vectors meaning they are pretty similar. 

Here's how to use vector differences for word analogies `x:y as z:`. Compute the vector difference between the first two words and then look for similar vector differences. The simplest mechanism is to exhaustively compare the x-y vector difference to the vector difference from z to all other words in the table. If we sort by distance, the first n words will be the most appropriate words to finish the analogy. Here is your code template for the function.

```python
def analogies(gloves, x, y, z, n):
    """
    Given a gloves dictionary of word:vector and 3 words from
    "x is to y as z is to _____", return the n best words that fill in
    the blank to complete the analogy.

    Compute the vector difference between x and y then compute the vector
    difference between z and all vectors, v, in gloves database. Track
    the distances with a list of tuples of the form: (distance, word).  Sort
    the list by distance. Return a list of the first n words from the sorted
    list. Do not return the tuples, just the words.
    Return a python list of strings not numpy array.
    """
    ...
```

### Where to go from here

If you are feeling particularly frisky near the end of the boot camp, you can do a nice visualization of word vectors. The idea is to take the very large 300-dimensional vectors and project them onto just 2-dimensional space so that we can plot them. The key to such a compression is to perform *principal components analysis* (PCA) on a set of word vectors, which you might hear about in the linear algebra boot camp. This is how I drew the graph above for the words king, queen, and cat (and 3 nearest neighbors). Here is some skeleton code for you to get started:

```python
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

def plot_words(gloves, words, n):
    """
    Get a list of vectors for n similar words for each w in words.
    Flatten that into a single list of vectors, including the original
    words' vectors.  Compute a word vector for each of those words and
    put into a list. Use PCA to project the vectors onto two dimensions.
    Extra separate X and Y coordinate lists and pass to matplotlib's scatter
    function. Then, iterate through the expanded word list and plot the
    string using text() with, say, fontsize=16. call show().
    """
    ...
    wvecs = ...
    pca = PCA(n_components=2)
    vecs2D = pca.fit_transform(wvecs)
    ...
```

For words `petal`, `software`, and `car` you should get:

<img src="figures/wordvec2.png" width=400>

## Deliverables

In your repository, you should submit file `wordsim.py` in the root directory containing all of the code described above, except for the extra plotting code (that you can do in a separate file). We will only be testing the nearest neighbor and word analogy functionality.

You can use numpy but please do not refer to a bunch of random packages that I probably don't have installed on my test box. Your test will fail.

## Evaluation

Please be aware that, depending on the hardware you run this on, the program could be fairly slow. On my iMac, the test described here takes 45 seconds, which includes time to load and process the 1G file containing 400,000 words.

We will run your program from the command line as follows using the 300-dimensional vectors:

```bash
$ python test_wordsim.py ~/data/glove/glove.6B.300d.txt 
All tests pass
$ 
```

If there is an error, you will see something like this:

```bash
$ python test_wordsim.py ~/data/glove/glove.6B.300d.txt 
similar words for dog should be ['dogs', 'cat', 'pet', 'puppy', 'hound'] but was ['poodle', 'cat', 'pet', 'puppy', 'hound']
$
```

Here is the test rig file `test_wordsim.py`:

```python
from wordsim import *

word_input = [
    'dog', 'cow', 'spain', 'king'
]
word_output = [
    ['dogs', 'cat', 'pet', 'puppy', 'hound'],
    ['cows', 'mad', 'bovine', 'sheep', 'goat'],
    ['portugal', 'spanish', 'morocco', 'madrid', 'spaniards'],
    ['queen', 'monarch', 'prince', 'kingdom', 'reign'],
]

analogy_input = [
    ['building', 'architect', 'software'],
    ['king', 'queen', 'man'],
    ['german', 'english', 'french'],
    ['spanish', 'spain', 'french']
]
analogy_output = [
    ['programmer', 'architect', 'designer', 'computer', 'microsoft'],
    ['woman', 'girl', 'person', 'teenager', 'she'],
    ['english', 'welsh', 'spanish', 'language', 'prohertrib'],
    ['france', 'belgium', 'paris', 'spain', 'prohertrib']
]

glove_filename = sys.argv[1] # must pass in the 300-Dimensional vectors
gloves = load_glove(glove_filename)

errors = []

for i,w in enumerate(word_input):
    closest = closest_words(gloves, w.lower(), 5)
    if closest != word_output[i]:
        errors.append("similar words for %s should be %s but was %s" % (w,word_output[i],closest))

for i,w in enumerate(analogy_input):
    analogs = analogies(gloves, w[0].lower(), w[1].lower(), w[2].lower(), 5)
    if analogs != analogy_output[i]:
        errors.append("analogy for %s should be %s but was %s" % (w,analogy_output[i],analogs))

if len(errors)>0:
    print '\n'.join(errors)
else:
    print "All tests pass"
```
